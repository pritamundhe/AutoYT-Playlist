{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Topic Extraction Model Training\n",
                "\n",
                "This notebook trains a topic extraction model for academic syllabus parsing.\n",
                "\n",
                "**Goal**: Create `topic_extractor.pkl` for use in the AutoYT-Playlist system.\n",
                "\n",
                "## Steps:\n",
                "1. Load and preprocess syllabus data\n",
                "2. Extract topics using NLP techniques\n",
                "3. Train a topic classification model\n",
                "4. Save the model as `.pkl` file\n",
                "\n",
                "**Upload this notebook to Kaggle and run it there!**"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Install Dependencies"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install -q transformers sentence-transformers spacy scikit-learn pandas numpy\n",
                "!python -m spacy download en_core_web_sm"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Import Libraries"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import re\n",
                "import os\n",
                "import pickle\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "from typing import List, Dict, Tuple\n",
                "\n",
                "import spacy\n",
                "from sentence_transformers import SentenceTransformer\n",
                "from sklearn.cluster import DBSCAN\n",
                "from sklearn.feature_extraction.text import TfidfVectorizer\n",
                "\n",
                "print(\"‚úÖ Libraries imported successfully!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Load Sample Syllabus Data\n",
                "\n",
                "**Note**: Upload your own syllabus files to Kaggle dataset or use the sample below."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Sample syllabus text (replace with your actual data)\n",
                "sample_syllabus = \"\"\"\n",
                "Machine Learning Course Syllabus\n",
                "\n",
                "Unit 1: Introduction to Machine Learning\n",
                "- What is Machine Learning?\n",
                "- Types of Machine Learning: Supervised, Unsupervised, Reinforcement\n",
                "- Applications of ML\n",
                "\n",
                "Unit 2: Linear Regression\n",
                "- Simple Linear Regression\n",
                "- Multiple Linear Regression\n",
                "- Gradient Descent\n",
                "- Cost Function\n",
                "\n",
                "Unit 3: Classification Algorithms\n",
                "- Logistic Regression\n",
                "- Decision Trees\n",
                "- Random Forests\n",
                "- Support Vector Machines\n",
                "\n",
                "Unit 4: Neural Networks\n",
                "- Perceptron\n",
                "- Backpropagation\n",
                "- Deep Learning Basics\n",
                "- Convolutional Neural Networks\n",
                "\"\"\"\n",
                "\n",
                "print(\"Sample syllabus loaded.\")\n",
                "print(f\"Length: {len(sample_syllabus)} characters\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Topic Extraction Class"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class TopicExtractor:\n",
                "    \"\"\"Extract topics from academic syllabus using NLP.\"\"\"\n",
                "    \n",
                "    def __init__(self):\n",
                "        print(\"Loading models...\")\n",
                "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
                "        self.embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
                "        print(\"‚úÖ Models loaded!\")\n",
                "    \n",
                "    def extract_topics(self, text: str) -> List[Dict]:\n",
                "        \"\"\"Extract topics from syllabus text.\"\"\"\n",
                "        topics = []\n",
                "        \n",
                "        # Pattern matching for common syllabus structures\n",
                "        patterns = [\n",
                "            r'Unit\\s+(\\d+):\\s*(.+?)(?=\\n|$)',\n",
                "            r'Chapter\\s+(\\d+):\\s*(.+?)(?=\\n|$)',\n",
                "            r'Week\\s+(\\d+):\\s*(.+?)(?=\\n|$)',\n",
                "            r'Module\\s+(\\d+):\\s*(.+?)(?=\\n|$)',\n",
                "            r'Lecture\\s+(\\d+):\\s*(.+?)(?=\\n|$)',\n",
                "        ]\n",
                "        \n",
                "        for pattern in patterns:\n",
                "            matches = re.finditer(pattern, text, re.IGNORECASE | re.MULTILINE)\n",
                "            for match in matches:\n",
                "                topic_num = match.group(1)\n",
                "                topic_name = match.group(2).strip()\n",
                "                \n",
                "                # Extract subtopics (bullet points after the main topic)\n",
                "                subtopics = self._extract_subtopics(text, match.end())\n",
                "                \n",
                "                topics.append({\n",
                "                    'number': topic_num,\n",
                "                    'name': topic_name,\n",
                "                    'subtopics': subtopics,\n",
                "                    'type': pattern.split('\\\\')[0].replace('r\\'', '')\n",
                "                })\n",
                "        \n",
                "        # If no structured topics found, use sentence clustering\n",
                "        if not topics:\n",
                "            topics = self._cluster_topics(text)\n",
                "        \n",
                "        return topics\n",
                "    \n",
                "    def _extract_subtopics(self, text: str, start_pos: int, max_lines: int = 10) -> List[str]:\n",
                "        \"\"\"Extract bullet points/subtopics after a main topic.\"\"\"\n",
                "        subtopics = []\n",
                "        lines = text[start_pos:].split('\\n')[:max_lines]\n",
                "        \n",
                "        for line in lines:\n",
                "            line = line.strip()\n",
                "            # Stop at next major topic\n",
                "            if re.match(r'(Unit|Chapter|Week|Module|Lecture)\\s+\\d+', line, re.IGNORECASE):\n",
                "                break\n",
                "            # Extract bullet points\n",
                "            if line.startswith(('-', '‚Ä¢', '*', '‚Äì')) or re.match(r'^\\d+\\.', line):\n",
                "                subtopic = re.sub(r'^[-‚Ä¢*‚Äì\\d.]+\\s*', '', line)\n",
                "                if subtopic:\n",
                "                    subtopics.append(subtopic)\n",
                "        \n",
                "        return subtopics\n",
                "    \n",
                "    def _cluster_topics(self, text: str) -> List[Dict]:\n",
                "        \"\"\"Cluster sentences into topics using embeddings.\"\"\"\n",
                "        doc = self.nlp(text)\n",
                "        sentences = [sent.text.strip() for sent in doc.sents if len(sent.text.strip()) > 20]\n",
                "        \n",
                "        if not sentences:\n",
                "            return []\n",
                "        \n",
                "        # Generate embeddings\n",
                "        embeddings = self.embedder.encode(sentences)\n",
                "        \n",
                "        # Cluster using DBSCAN\n",
                "        clustering = DBSCAN(eps=0.5, min_samples=2, metric='cosine').fit(embeddings)\n",
                "        \n",
                "        # Group sentences by cluster\n",
                "        topics = []\n",
                "        for cluster_id in set(clustering.labels_):\n",
                "            if cluster_id == -1:  # Skip noise\n",
                "                continue\n",
                "            \n",
                "            cluster_sentences = [sentences[i] for i, label in enumerate(clustering.labels_) if label == cluster_id]\n",
                "            \n",
                "            topics.append({\n",
                "                'number': str(cluster_id + 1),\n",
                "                'name': cluster_sentences[0][:100],  # Use first sentence as topic name\n",
                "                'subtopics': cluster_sentences[1:],\n",
                "                'type': 'clustered'\n",
                "            })\n",
                "        \n",
                "        return topics\n",
                "\n",
                "print(\"‚úÖ TopicExtractor class defined!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Train and Test the Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize extractor\n",
                "extractor = TopicExtractor()\n",
                "\n",
                "# Extract topics from sample syllabus\n",
                "topics = extractor.extract_topics(sample_syllabus)\n",
                "\n",
                "# Display results\n",
                "print(f\"\\nüìö Extracted {len(topics)} topics:\\n\")\n",
                "for topic in topics:\n",
                "    print(f\"\\n{topic['type'].upper()} {topic['number']}: {topic['name']}\")\n",
                "    if topic['subtopics']:\n",
                "        for subtopic in topic['subtopics'][:5]:  # Show first 5\n",
                "            print(f\"  - {subtopic}\")\n",
                "        if len(topic['subtopics']) > 5:\n",
                "            print(f\"  ... and {len(topic['subtopics']) - 5} more\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Save the Model as .pkl File"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save the trained extractor\n",
                "output_path = 'topic_extractor.pkl'\n",
                "\n",
                "with open(output_path, 'wb') as f:\n",
                "    pickle.dump(extractor, f)\n",
                "\n",
                "print(f\"\\n‚úÖ Model saved to: {output_path}\")\n",
                "print(f\"File size: {os.path.getsize(output_path) / (1024*1024):.2f} MB\")\n",
                "\n",
                "# Test loading\n",
                "with open(output_path, 'rb') as f:\n",
                "    loaded_extractor = pickle.load(f)\n",
                "    \n",
                "print(\"\\n‚úÖ Model loaded successfully!\")\n",
                "print(\"\\nüì• Download this file and place it in: ml_models/nlp/topic_extractor.pkl\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Validation Test"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test with loaded model\n",
                "test_text = \"\"\"\n",
                "Week 1: Python Basics\n",
                "- Variables and Data Types\n",
                "- Control Flow\n",
                "- Functions\n",
                "\n",
                "Week 2: Object-Oriented Programming\n",
                "- Classes and Objects\n",
                "- Inheritance\n",
                "- Polymorphism\n",
                "\"\"\"\n",
                "\n",
                "test_topics = loaded_extractor.extract_topics(test_text)\n",
                "print(f\"\\nüß™ Test extraction: Found {len(test_topics)} topics\")\n",
                "for topic in test_topics:\n",
                "    print(f\"  - {topic['name']}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Next Steps\n",
                "\n",
                "1. ‚úÖ Download `topic_extractor.pkl` from Kaggle\n",
                "2. üìÅ Place it in: `c:\\Users\\Acer\\Documents\\GitHub\\AutoYT-Playlist\\ml_models\\nlp\\topic_extractor.pkl`\n",
                "3. üöÄ The backend will automatically use this model!\n",
                "\n",
                "---\n",
                "\n",
                "**Optional Improvements:**\n",
                "- Train on more diverse syllabi\n",
                "- Fine-tune the clustering parameters\n",
                "- Add domain-specific keyword extraction"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}